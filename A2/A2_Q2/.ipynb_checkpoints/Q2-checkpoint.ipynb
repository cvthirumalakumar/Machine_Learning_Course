{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518efe1c-da13-465c-b62a-0e38ae6f110c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SMAI Assignment - 2\n",
    "\n",
    "## Question 2: Gaussian Mixture Models\n",
    "\n",
    "Resources: \n",
    "- https://youtu.be/qMTuMa86NzU\n",
    "- https://youtu.be/ZBLyXgjBx3Q\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/mixture.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e7d9be-241c-4e93-aefe-36d5ffddb4df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a78b0c-256b-4b46-b925-d69e455e3d76",
   "metadata": {},
   "source": [
    "### Part 1: Gaussian Mixture Models\n",
    "\n",
    "We'll attempt to solve the task of background subtraction using Gaussian Mixture Models. Before that, you will need to implement the Gaussian Mixture Model algorithm from scratch.\n",
    "\n",
    "Some details: \n",
    "- Try to implement GMMs using Multi-variate Gaussian Distributions, the following tasks in the assignment are possible to implement using the Univariate version too but it might be bit inaccurate as explained below.\n",
    "    - To clarify further, we could treat each pixel in our RGB image as our data point with [R, G, B] channels as the dimensions to the Multi-variate data point, and we would easily get predictions for each pixel location using Multi-variate approach.\n",
    "    - Or, we could treat every single value in the given RGB image as a data point independent of what channel the belong to and consider them as Uni-variate data point, and get prediction using the Uni-variate approach.\n",
    "    But this affects our prediction, since we can't simply make per pixel predtions anymore, because for every pixel location we would now have 3 different predictions.\n",
    "    - To get around this, you could convert your image to Grayscale and then we would only have one channel/value corresponding to each pixel location, which would now allow us to use the Uni-variate approach for prediction, but this also means loss in information which would affect our quality of predictions.\n",
    "    - Try to have a class based implementation of GMM, this would really help you in Background Subtraction task. You can get some general ideas on how to structure your class by looking at `sklearn.mixture.GaussianMixture` documentation and source code.\n",
    "- The following code cell has a rough template to get you started with the implementation. You are free to change the structure of the code, this is just a suggestion to help you get started.\n",
    "\n",
    "\n",
    "TLDR: You may implement the univariate version of GMMs, but it might not be as accurate as the multivariate version and it is recommended to try and implement the multivariate version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8b4171-3b73-4e4b-b123-29b20235002e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    \n",
    "    weights = None\n",
    "    means = None\n",
    "    covars = None\n",
    "    \n",
    "    def __init__(self, n_components=1, tol=1e-3, max_iter=100):\n",
    "        \"\"\"\n",
    "        n_components: The number of mixture components.\n",
    "        tol: The convergence threshold\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.reg_covar = 1e-6\n",
    "\n",
    "    def initialize_params(self, X):\n",
    "        \"\"\"\n",
    "        X : A collection of `N` training data points, each with dimension `d`.\n",
    "        \"\"\"\n",
    "        n_samples, self.d = X.shape\n",
    "        # rand_indices = np.random.choice(X.shape[0], size=self.n_components, replace=False)\n",
    "        # self.means = X[rand_indices]\n",
    "        kmeans = KMeans(n_clusters=self.n_components,n_init='auto',init='k-means++')\n",
    "        kmeans.fit(X)\n",
    "        self.means = kmeans.cluster_centers_\n",
    "        variances = np.var(X, axis=0)\n",
    "        self.covars = np.zeros((self.n_components, self.d, self.d))\n",
    "        for k in range(self.n_components):\n",
    "            np.fill_diagonal(self.covars[k], variances)\n",
    "        self.weights = np.ones(self.n_components) / self.n_components\n",
    "        \n",
    "    \n",
    "    def E_step(self, X):\n",
    "        \"\"\"\n",
    "        Find the Expectation of the log-likelihood evaluated using the current estimate for the parameters.\n",
    "        \"\"\"\n",
    "        posteriors = np.zeros((X.shape[0], self.n_components))\n",
    "        for i in range(self.n_components):\n",
    "            posteriors[:,i] = np.log(self.weights[i])+self.log_multivariate_normal(X, self.means[i], self.covars[i])\n",
    "        # print(posteriors)\n",
    "        log_likelihoods = np.log(np.sum(np.exp(posteriors), axis=1))\n",
    "        posteriors -= log_likelihoods[:, np.newaxis]\n",
    "        self.posteriors = posteriors\n",
    "\n",
    "    def log_multivariate_normal(self, X, mean, covariance):\n",
    "   \n",
    "        n_samples = X.shape[1]\n",
    "        log_det = np.log(np.linalg.det(covariance)+ 1e-6)\n",
    "        log_norm = -0.5 * (n_samples * np.log(2 * np.pi) + log_det)\n",
    "        deviation = X - mean\n",
    "        log_exponent = -0.5 * np.sum(deviation @ np.linalg.inv(covariance) * deviation, axis=1)\n",
    "        # print(log_norm + log_exponent)\n",
    "        return log_norm + log_exponent\n",
    "        \n",
    "    def M_step(self, X):\n",
    "        \"\"\"\n",
    "        Updates parameters maximizing the expected log-likelihood found on the E step.\n",
    "        \"\"\"\n",
    "        self.posteriors = np.exp(self.posteriors)\n",
    "        sum_of_posterios = self.posteriors.sum(axis=0)\n",
    "        self.means = np.dot(self.posteriors.T, X) / sum_of_posterios[:, np.newaxis]\n",
    "        for i in range(self.n_components):\n",
    "            deviation = X - self.means[i]\n",
    "            self.covars[i] = np.dot(self.posteriors[:, i] * deviation.T, deviation) / sum_of_posterios[i]\n",
    "            self.covars[i].flat[:: self.d + 1] += self.reg_covar\n",
    "        self.weights = sum_of_posterios / sum_of_posterios.sum()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the parameters of the GMM on some training data.\n",
    "        \"\"\"\n",
    "        self.initialize_params(X)\n",
    "        prev_log_likelihood = None\n",
    "        self.iterations = 0\n",
    "        for _ in range(self.max_iter):\n",
    "            self.iterations +=1\n",
    "            self.E_step(X)\n",
    "            self.M_step(X)\n",
    "            self.E_step(X)\n",
    "            log_likelihood = np.sum(self.posteriors)\n",
    "            if prev_log_likelihood is not None and np.abs(log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                break\n",
    "            prev_log_likelihood = log_likelihood\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels for the data samples in X using trained model.\n",
    "        \"\"\"\n",
    "        self.E_step(X)\n",
    "        preds = np.argmax(self.posteriors, axis=1)\n",
    "\n",
    "        # Create binary labels indicating the component with the highest weight\n",
    "        binary_labels = np.zeros_like(preds)\n",
    "        for i in range(self.n_components):\n",
    "            binary_labels[preds == i] = int(i == np.argmax(self.weights))\n",
    "    \n",
    "        return binary_label\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a6266-8fe6-4b8a-8f40-5e0e3c8721c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 2: Background Subtraction\n",
    "\n",
    "![traffic](./videos/traffic.gif)\n",
    "\n",
    "In this question, you are required to extract the background image from a given set of training frames, and use the extracted background to display foreground objects in the test frames by subtracting that background image and then thresholding it accordingly.\n",
    "\n",
    "In this question, we are going to try different baselines to extract background from low resolution camera footage:\n",
    "\n",
    "1. Frame Averaging:\n",
    "    - Just take the average of every training frame, which gives us an approximate background image.\n",
    "    \n",
    "2. GMM Per Pixel:\n",
    "    - We will maintain per pixel GMMs of 2 components, and then fit these GMMs considering every training from for its corresponding pixel.\n",
    "    - And then use these GMMs to predict the pixel labels for every subsequent frame.\n",
    "    - Most of the time, the Gaussian with the higher weight corresponds to the background.\n",
    "    - We can implement this in a simpler way but with worse prediction results, you can extract a mean background image similar to the first baseline above.\n",
    "    - To extract the Mean background image, we can assign values of the Means corresponding to the highest weighted Gaussian for each pixel.\n",
    "    - This method is much simpler to implement but, this could give worse results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05a728-7e46-4875-8202-a2f4c15474ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Extracting Frames from videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bce85da-089c-49cb-a145-262f888e16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = 'videos'\n",
    "video = 'traffic.gif'\n",
    "\n",
    "source_path = f'./{source_folder}/{video}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ff69e5-55c6-4351-ba5c-4f09f1affe4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = 'frames'\n",
    "\n",
    "frames_path = f\"./{data_folder}/{video.rsplit('.', 1)[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec1404b3-0318-4d1d-a4f8-cd1ad32c312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!mkdir -p {frames_path} > /dev/null ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc493fa5-d2fb-4354-8eec-aa3daa3e3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!ffmpeg -i {source_path} {frames_path}/'frame_%04d.png' > /dev/null ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121347b-6089-4cf0-bc6e-1a341a48c397",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loading Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9989bdab-245a-4c67-bc3e-c2067e9a72ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "frames = []\n",
    "\n",
    "for file_path in sorted(glob.glob(f'{frames_path}/*.png', recursive = False)):\n",
    "    img = cv2.imread(file_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = np.asarray(img, dtype=np.float64)\n",
    "    img /= 255.0\n",
    "    \n",
    "    frames.append(img)\n",
    "    \n",
    "frames = np.asarray(frames, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e589fc-902a-419d-8c9a-13311fcab7ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f859993a-2b29-4baf-a168-117151ed240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame: (80, 120, 160, 3)\n",
      "train_frames: (48, 120, 160, 3)\n",
      "test_frames: (32, 120, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f'frame: {frames.shape}')\n",
    "\n",
    "train_frames, test_frames = train_test_split(frames, train_size=0.6, shuffle=False) # Do Not Shuffle!\n",
    "\n",
    "print(f'train_frames: {train_frames.shape}')\n",
    "print(f'test_frames: {test_frames.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec3f0d8",
   "metadata": {},
   "source": [
    "Note: You may use helper libraries like `imageio` for working with GIFs.\n",
    "\n",
    "```python\n",
    "import imageio\n",
    "\n",
    "def make_gif(img_list, gif_path, fps=10):\n",
    "    imageio.mimsave(gif_path, img_list, fps=fps) \n",
    "    return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cebb520-1657-432d-959d-eec893cb9b44",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Frame Averaging\n",
    "\n",
    "Extract Background Image from the training data and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d08a0a6-770f-462a-8418-a63b6ffec5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "avg_background = np.mean(train_frames, axis=0)\n",
    "cv2.imshow('Average background',avg_background)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b08135-acc3-4a7f-8d9e-11d9d2e15507",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### GMMs per pixel\n",
    "\n",
    "Create Set of GMMs for every pixel and fit them considering every training frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "630bb208-e186-4892-aea3-41082bd52789",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████████████████████████████████████████████████████████████████████████████▊                                               | 12000/19200 [10:06<06:04, 19.76it/s]/home/kumar/.local/lib/python3.10/site-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 19199/19200 [16:51<00:00, 28.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 16.86577180226644 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19200/19200 [17:10<00:00, 28.63it/s]"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "start_time = time.time()\n",
    "gmm_grid = [[None] * train_frames.shape[2] for _ in range(train_frames.shape[1])]\n",
    "total_iterations = train_frames.shape[1]*train_frames.shape[2]\n",
    "progress_bar = tqdm(total=total_iterations)\n",
    "for i in range(train_frames.shape[1]):\n",
    "    for j in range(train_frames.shape[2]):\n",
    "        gmm = GMM(n_components=2)\n",
    "        gmm.fit(train_frames[:,i,j,:])\n",
    "        gmm_grid[i][j] = gmm\n",
    "        progress_bar.update(1)\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)/60\n",
    "print(\"Elapsed time:\", elapsed_time, \"minutes\")\n",
    "\n",
    "# data = train_frames[:,1,1,:]\n",
    "# data.shape\n",
    "# gmm = GMM(n_components=2)\n",
    "# gmm.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046f7c9-1950-4704-a62a-f0f27079f95d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Extract Background Image from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "511078b9-35a9-4712-9e54-a7248187c7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "average_gmm = np.zeros((train_frames.shape[1],train_frames.shape[2],3))\n",
    "for i in range(train_frames.shape[1]):\n",
    "    for j in range(train_frames.shape[2]):\n",
    "        gmm = gmm_grid[i][j]\n",
    "        max_component_idx = np.argmax(gmm.weights)\n",
    "        average_gmm[i,j,:] = gmm.means[max_component_idx]\n",
    "cv2.imshow('GMM background',average_gmm)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f029f-c0d6-4c33-b096-dedf4b389295",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Outputs\n",
    "\n",
    "You can use the helper functions given below to display and save frames as videos, feel free to change them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7013953b-371b-4111-ba54-cd630b64bec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def display_frames(frames, fps=10.0):\n",
    "    \"\"\"\n",
    "    Display the frames as a video.\n",
    "    \"\"\"\n",
    "    eps = 0.0001\n",
    "    \n",
    "    wait_time = int(1000 // fps)\n",
    "    \n",
    "    for frame in frames:\n",
    "        frame = frame.astype(np.float64)\n",
    "        frame = (frame - frame.min()) * 255 / (frame.max() - frame.min() + eps)\n",
    "        frame = frame.astype(np.uint8)\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        cv2.imshow(\"video\", frame)\n",
    "        k = cv2.waitKey(wait_time)\n",
    "\n",
    "        if k == ord('q'):\n",
    "            print(\"Quitting...\")\n",
    "            break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def save_frames(frames, fps=10.0, output_path='./results', file_name='temp'):\n",
    "    \"\"\"\n",
    "    Save the frames as a video.\n",
    "    \"\"\"\n",
    "    eps = 0.0001\n",
    "    \n",
    "    frame_rate = float(fps)\n",
    "    frame_size = (int(frames[0].shape[1]), int(frames[0].shape[0]))\n",
    "    wait_time = int(1000 // fps)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    \n",
    "    save_path = os.path.join(output_path, f\"{file_name.rsplit('.', 1)[0]}.mp4\")\n",
    "    \n",
    "    vid_wrt = cv2.VideoWriter(save_path, fourcc, frame_rate, frame_size)\n",
    "\n",
    "    for frame in frames:\n",
    "        frame = frame.astype(np.float64)\n",
    "        frame = (frame - frame.min()) * 255 / (frame.max() - frame.min() + eps)\n",
    "        frame = frame.astype(np.uint8)\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(wait_time) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "        vid_wrt.write(frame)\n",
    "\n",
    "        \n",
    "    vid_wrt.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68505dec-141f-463d-be72-020fca8e98da",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Frame Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "797fa50a-8d81-4afa-affd-68bb9a10cb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your output here\n",
    "frames_bgremoved_avgbg = []\n",
    "for frame in test_frames:\n",
    "    frames_bgremoved_avgbg.append(frame-avg_background)\n",
    "display_frames(frames_bgremoved_avgbg)\n",
    "save_frames(frames_bgremoved_avgbg,file_name='average_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c5b0f-9e30-44f6-88dc-8efb91e588a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### GMMs per pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0603479f-0bb2-4cab-94c0-9576b99ee0cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your output here\n",
    "frames_bgremoved_gmmbg = []\n",
    "for frame in test_frames:\n",
    "    frames_bgremoved_gmmbg.append(frame-average_gmm)\n",
    "display_frames(frames_bgremoved_gmmbg)\n",
    "save_frames(frames_bgremoved_gmmbg,file_name='gmm_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44faa83f-cae3-4acc-987e-85c9e454da6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving background images\n",
    "avg_background_255 = (avg_background * 255)\n",
    "cv2.imwrite('./results/average_background.png',avg_background_255)\n",
    "average_gmm_255 = (average_gmm * 255)\n",
    "cv2.imwrite('./results/gmm_background.png',average_gmm_255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39acbb-e005-4335-8d74-b7e75dfca2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
