{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280a412b-a17c-48fa-8784-0cc2e24b2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d2a0b3-e9f8-4a68-8539-73b72f6367fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = torchvision.datasets.CIFAR10(root='data', train=True,download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='data', train=False, download=True,transform=transform)\n",
    "\n",
    "train_size, val_size = 0.9, 0.1\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643581c-42c6-4bc4-805a-3055bfad4fb7",
   "metadata": {},
   "source": [
    "# MLP\n",
    "Training MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d8958e1-02e9-47a3-b515-953e4db7bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 216)\n",
    "        self.fc3 = nn.Linear(216, 128)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint( model)\n",
    "        elif score < self.best_score - self.delta:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77bfc177-7d4e-4ec3-8038-68eb1a65fec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilising cuda\n",
      "Training........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 23.15it/s, loss=1.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.6627673157914118, Val Loss: 1.5720328092575073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 23.35it/s, loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.4414105567742477, Val Loss: 1.46636164188385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 23.35it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.3256484534252773, Val Loss: 1.4156811237335205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 23.32it/s, loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 1.226941398937594, Val Loss: 1.4045324325561523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 23.41it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 1.1334933440454982, Val Loss: 1.3792709112167358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 23.39it/s, loss=0.853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 1.0518058154054664, Val Loss: 1.431624412536621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 23.41it/s, loss=0.878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.9649936231699857, Val Loss: 1.4155257940292358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 23.32it/s, loss=0.949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.8866768653758548, Val Loss: 1.4494661092758179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:14<00:00, 23.73it/s, loss=0.975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.8151913706890561, Val Loss: 1.5312902927398682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:14<00:00, 23.70it/s, loss=0.965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.7438181504945863, Val Loss: 1.5877808332443237\n",
      "Early stopping....\n",
      "Best val loss is tensor(1.3793, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Training MLP\n",
    "model = MLP()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Utilising\",device)\n",
    "model = model.to(device)\n",
    "model_path = 'models/mlp.pt'\n",
    "early_stopping = EarlyStopping(patience=5, path=model_path)\n",
    "\n",
    "print(\"Training........\")\n",
    "for epoch_num in range(50):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch_num}\", total=len(train_loader))\n",
    "\n",
    "    for batch_num, (input, labels) in enumerate(progress_bar):\n",
    "        (input, labels) = (input.to(device), labels.to(device))\n",
    "        pred = model(input)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_num, (words, tags) in enumerate(val_loader):\n",
    "            (words, tags) = (words.to(device), tags.to(device))\n",
    "            pred = model(words)\n",
    "            val_loss += loss_fn(pred, tags)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    if epoch_num % 1 == 0:\n",
    "        print(f\"Epoch {epoch_num}, Train Loss: {epoch_loss}, Val Loss: {val_loss}\")\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping....\")\n",
    "        break\n",
    "print(\"Best val loss is\",early_stopping.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7eb5e8e-8415-4ea2-824b-e9475b77a1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing......\n",
      "Utilising cuda\n",
      "Test set accuracy 0.5337\n"
     ]
    }
   ],
   "source": [
    "# Testing MLP\n",
    "print(\"Testing......\")\n",
    "model = MLP()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Utilising\",device)\n",
    "model = model.to(device)\n",
    "model_path = 'models/mlp.pt'\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    all_ground_truth = []\n",
    "    for batch_num, (input, labels) in enumerate(test_loader):\n",
    "        (input, labels) = (input.to(device), labels.to(device))\n",
    "        pred = model(input)\n",
    "        predictions = torch.argmax(pred, axis=-1).cpu().numpy()\n",
    "        ground_truth = labels.cpu().numpy()\n",
    "        all_predictions.append(predictions)\n",
    "        all_ground_truth.append(ground_truth)\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_ground_truth = np.concatenate(all_ground_truth)\n",
    "    accuracy_test = accuracy_score(all_ground_truth, all_predictions)\n",
    "print(f\"Test set accuracy {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24411c1f-c51e-451b-bee2-18f7dc445461",
   "metadata": {},
   "source": [
    "# Observation\n",
    "1. Best Train loss is 1.1334 and Best Val loss is 1.3793\n",
    "2. Test accuracy is 53.37%\n",
    "3. The loss is not converged to a better minima as a result accuracy is less.\n",
    "4. This is because MLP has many parameters when flattened the images, which requires lot of data to perform well. That will be reason for lower performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc766db2-76cb-46de-8334-8c958b944d86",
   "metadata": {},
   "source": [
    "# CNN\n",
    "Training CNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606f601d-ccea-427a-a6f3-2196e22f805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c89ac86d-ca7f-400d-9139-dcae92b2e04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilising cuda\n",
      "Training........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 22.15it/s, loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.3910555086013945, Val Loss: 1.1970425844192505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 22.25it/s, loss=0.807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.0022837788882581, Val Loss: 1.0100511312484741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 22.26it/s, loss=0.678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.821267903867093, Val Loss: 0.9200318455696106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 22.24it/s, loss=0.554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.6853613330220635, Val Loss: 0.850812554359436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 22.19it/s, loss=0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.5412718580019745, Val Loss: 0.861900806427002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 22.27it/s, loss=0.578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.4100508125160228, Val Loss: 0.8703452944755554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 22.28it/s, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.28367172786965966, Val Loss: 0.9767342805862427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 22.27it/s, loss=0.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.17630035373043607, Val Loss: 1.1386831998825073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:15<00:00, 22.27it/s, loss=0.108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.1064487252574922, Val Loss: 1.1834677457809448\n",
      "Early stopping....\n",
      "Best val loss is tensor(0.8508, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Utilising\",device)\n",
    "model = model.to(device)\n",
    "model_path = 'models/cnn.pt'\n",
    "early_stopping = EarlyStopping(patience=5, path=model_path)\n",
    "print(\"Training........\")\n",
    "\n",
    "for epoch_num in range(50):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch_num}\", total=len(train_loader))\n",
    "\n",
    "    for batch_num, (input, labels) in enumerate(progress_bar):\n",
    "        (input, labels) = (input.to(device), labels.to(device))\n",
    "        pred = model(input)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_num, (words, tags) in enumerate(val_loader):\n",
    "            (words, tags) = (words.to(device), tags.to(device))\n",
    "            pred = model(words)\n",
    "            val_loss += loss_fn(pred, tags)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    if epoch_num % 1 == 0:\n",
    "        print(f\"Epoch {epoch_num}, Train Loss: {epoch_loss}, Val Loss: {val_loss}\")\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping....\")\n",
    "        break\n",
    "print(\"Best val loss is\",early_stopping.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86eaab02-8f7d-4d7a-b59f-1871953ccaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing......\n",
      "Utilising cuda\n",
      "Test set accuracy 0.7091\n"
     ]
    }
   ],
   "source": [
    "# Testing CNN\n",
    "print(\"Testing......\")\n",
    "model = CNN()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Utilising\",device)\n",
    "model = model.to(device)\n",
    "model_path = 'models/cnn.pt'\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    all_ground_truth = []\n",
    "    for batch_num, (input, labels) in enumerate(test_loader):\n",
    "        (input, labels) = (input.to(device), labels.to(device))\n",
    "        pred = model(input)\n",
    "        predictions = torch.argmax(pred, axis=-1).cpu().numpy()\n",
    "        ground_truth = labels.cpu().numpy()\n",
    "        all_predictions.append(predictions)\n",
    "        all_ground_truth.append(ground_truth)\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_ground_truth = np.concatenate(all_ground_truth)\n",
    "    accuracy_test = accuracy_score(all_ground_truth, all_predictions)\n",
    "print(f\"Test set accuracy {accuracy_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83057cf2-5eac-48a3-8712-f90162d5ca7f",
   "metadata": {},
   "source": [
    "# Observations\n",
    "1. Best train loss is 0.6853 and Best validation loss is 0.8508\n",
    "2. Test accuracy is 70.91%, which is 17.54% gretaer than MLP. Which is very signifcant improvement.\n",
    "3. When compared with MLP, CNNs have fewer parameters and the ability of learning low level feature maps like patterns, edges etc.. are advantage of CNN and can obtain good performance with the same data than MLP.\n",
    "4. Leaning of several spacial low level feature maps such as edges, small fileters of shapes, etc required for the task by learning jointly helps CNN to perform better than MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ff7a6-4e74-42ef-b795-a7dcc2eee1df",
   "metadata": {},
   "source": [
    "# Transfer Learning using VGG-16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c889f312-b5a0-44af-b56e-6a496b7c20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "vgg16 = models.vgg16(weights='DEFAULT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e423db8f-21c9-4d88-8302-f1c5f90794dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = vgg16.classifier[6].in_features\n",
    "vgg16.classifier[6] = nn.Linear(num_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a37c9b22-1120-42c7-ad1f-cf699e757816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilising cuda\n",
      "Training........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:48<00:00,  7.19it/s, loss=0.544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.8615189748392864, Val Loss: 0.5843836069107056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:49<00:00,  7.18it/s, loss=0.489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.5022154474122957, Val Loss: 0.49446535110473633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:49<00:00,  7.15it/s, loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.39050354749302973, Val Loss: 0.47516295313835144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:49<00:00,  7.15it/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.31310246825556864, Val Loss: 0.4596821963787079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:49<00:00,  7.15it/s, loss=0.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.24238071899691765, Val Loss: 0.455503910779953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:49<00:00,  7.18it/s, loss=0.0877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.19001077835194088, Val Loss: 0.4467201828956604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:49<00:00,  7.14it/s, loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.14511817769909446, Val Loss: 0.473048597574234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:49<00:00,  7.17it/s, loss=0.0935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.11024726475906474, Val Loss: 0.5000740885734558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:49<00:00,  7.16it/s, loss=0.0758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.08315005591827106, Val Loss: 0.5318211913108826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:49<00:00,  7.15it/s, loss=0.0862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.061156463825186205, Val Loss: 0.6018524765968323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:49<00:00,  7.16it/s, loss=0.0598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.053251084726070985, Val Loss: 0.5924177765846252\n",
      "Early stopping....\n",
      "Best val loss is tensor(0.4467, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = vgg16\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Utilising\",device)\n",
    "model = model.to(device)\n",
    "model_path = 'models/vgg16.pt'\n",
    "early_stopping = EarlyStopping(patience=5, path=model_path)\n",
    "\n",
    "print(\"Training........\")\n",
    "for epoch_num in range(50):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch_num}\", total=len(train_loader))\n",
    "\n",
    "    for batch_num, (input, labels) in enumerate(progress_bar):\n",
    "        (input, labels) = (input.to(device), labels.to(device))\n",
    "        pred = model(input)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_num, (words, tags) in enumerate(val_loader):\n",
    "            (words, tags) = (words.to(device), tags.to(device))\n",
    "            pred = model(words)\n",
    "            val_loss += loss_fn(pred, tags)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    if epoch_num % 1 == 0:\n",
    "        print(f\"Epoch {epoch_num}, Train Loss: {epoch_loss}, Val Loss: {val_loss}\")\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping....\")\n",
    "        break\n",
    "print(\"Best val loss is\",early_stopping.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edbd03d7-978f-4a25-a149-3a21165d767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing......\n",
      "Utilising cuda\n",
      "Test set accuracy 0.8636\n"
     ]
    }
   ],
   "source": [
    "# Testing VGG\n",
    "print(\"Testing......\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Utilising\",device)\n",
    "model = vgg16\n",
    "model = model.to(device)\n",
    "model_path = 'models/vgg16.pt'\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    all_ground_truth = []\n",
    "    for batch_num, (input, labels) in enumerate(test_loader):\n",
    "        (input, labels) = (input.to(device), labels.to(device))\n",
    "        pred = model(input)\n",
    "        predictions = torch.argmax(pred, axis=-1).cpu().numpy()\n",
    "\n",
    "\n",
    "        ground_truth = labels.cpu().numpy()\n",
    "        all_predictions.append(predictions)\n",
    "        all_ground_truth.append(ground_truth)\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_ground_truth = np.concatenate(all_ground_truth)\n",
    "    accuracy_test = accuracy_score(all_ground_truth, all_predictions)\n",
    "print(f\"Test set accuracy {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02308c5b-b53b-4fa0-90c9-eb9f5e259244",
   "metadata": {},
   "source": [
    "# Observations\n",
    "1. Best train loss is 0.1900 and best validation loss is 0.4467, these losses are far better than MLP and CNN.\n",
    "2. Test accuracy is 86.36%, which is 15.45% better than CNN and 32.99% better than MLP\n",
    "3. Transfer learning can benefit in better parameter initialisation. As the base model is trained with large dataset for similar task which can boost the convergence speed as well as performance with little finetuning data. And also the model already has the knowledge of capturing essential features, so can perform well with little finetuning data for relevant task.\n",
    "4. Time required is same/lesser than MLP and CNN, but reached loss which is far better than MLP and CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da06397-e184-4757-bc41-8a62c84e22a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
